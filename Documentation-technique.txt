 
	Image								Image 

 

Documentation Technique 

Choix de Mod√©lisation et Principes Adopt√©s 

 

Projet : IA-DAS 

Date : 27/05/2025 

Auteur : Sara TAOUFIQ 

Equipe : Stephanie Meriaux , Meggy Hayotte , Molka TOUNSI , Amandine DAUBRESSE 

 

Objectif :  

Ce document a pour vocation de d√©tailler les choix de mod√©lisation techniques et m√©thodologiques adopt√©s dans le cadre du d√©veloppement d‚Äôune ontologie d√©di√©e √† la repr√©sentation des Attitudes et Comportements Alimentaires Dysfonctionnels (ACAD) dans le contexte sportif. 

L'objectif principal de cette ontologie est de formaliser, structurer et interconnecter les connaissances issues d‚Äôarticles scientifiques portant sur les comportements alimentaires √† risque chez les populations sportives, afin de pouvoir interroger cette ontologie, croiser les variables d‚Äôint√©r√™t (ex. genre, type de sport, variables psychologiques) et en extraire des connaissances exploitables pour la recherche, la pr√©vention ou l‚Äôanalyse automatis√©e. 

 

 

Public cible : 

Cette documentions est destin√©e √† :  

L‚Äô√©quipe de recherche sur les ACAD et la psychologie du sport.  

Les d√©veloppeurs impliqu√©s dans la formalisation de l‚Äôontologie.  

Les experts en mod√©lisation des connaissances.  

Les futurs contributeurs souhaitant comprendre les bases de l‚Äôontologie d√©velopp√©e. 

 

Lien utile : 

Lien vers le repository, vous y trouverez le code, les Milestones ainsi que les issues associ√©es. 

Le repo est encore en priv√©, je ne peux renseigner le lien maintenant.   

 

Sommaire :  

 

Pr√©requis 

Glossaire 

P√©rim√®tre fonctionnel : Limites, Extension. 

Hypoth√®ses de travail  

Points non impl√©ment√©s relativement la sp√©cification et aux extensions requises  

 

Conception et architecture 

Choix des classes  

Ontologie pattern  

 

 Normalisation des donn√©es 

Methodologie ( Python/VBA Excel) 

Data Cleaning 

Data Profiling 

Feature Engeneering 

TroubleShoot en rapport avec l‚Äôontologie 

Donn√©es CSV  

Perte d‚Äôanalyse  

Debbug RML  

Debbug Fuseki 

Random errors and there solutions 

Mapping RML 

Sc√©nario complet 

Sc√©nario d‚Äôutilisation 1  

Sc√©nario d‚Äôutilisation 2 

Interface utilisateur 

Architecture 

Fonctionnalit√©  

Vid√©o de d√©monstration 

TroubleShoot En rapport avec l‚Äôinterface 

Changement de donn√©es docker  

Donn√©es non r√©cup√©r√©es 

Erreur r√©seau interface sur AWS 

Choix du desk pilote 


Glossaire 

Ontologie ‚Äì Concepts fondamentaux 

rdfs:Class : Repr√©sente la classe de toutes les classes. Toute nouvelle cat√©gorie de ressources dans l‚Äôontologie doit √™tre d√©finie comme une instance de rdfs:Class. 

rdf:Property : Utilis√©e pour d√©finir des propri√©t√©s, c‚Äôest-√†-dire des relations ou attributs reliant des ressources entre elles, ou √† des valeurs litt√©rales. 

rdfs:Resource : Classe englobante de toutes les ressources. Toute entit√© identifi√©e par un URI est consid√©r√©e comme une instance de rdfs:Resource. 

rdfs:Literal : Repr√©sente l‚Äôensemble des valeurs litt√©rales, comme les cha√Ænes de caract√®res, les nombres, les dates, etc. Ces valeurs ne sont pas des ressources mais des donn√©es brutes. 

Ontologie ‚Äì Pattern 

Normalisation ‚Äì Concepts cl√©s 

Data Cleaning : Processus consistant √† corriger ou supprimer les erreurs dans les donn√©es, telles que les doublons, les valeurs manquantes, les incoh√©rences de format, ou les fautes typographiques. Cette √©tape est essentielle pour garantir la qualit√© et la fiabilit√© des donn√©es avant leur transformation en RDF. 

Data Profiling : Analyse statistique et structurelle des donn√©es visant √† identifier les motifs r√©currents, les anomalies, les distributions de valeurs et les irr√©gularit√©s. Le profiling permet de d√©finir des r√®gles de validation ou de nettoyage adapt√©es au contexte du jeu de donn√©es. 

Canonicalisation : Transformation de diff√©rentes variantes d‚Äôune donn√©e vers une forme unique (exemple : ‚Äúh‚Äù devient ‚Äúheures‚Äù, ou inversement, selon la d√©cision prise). 

Harmonisation : 
Alignement de donn√©es issues de diff√©rentes sources selon un sch√©ma commun, afin d‚Äôassurer la coh√©rence et l‚Äôinterop√©rabilit√© des jeux de donn√©es. 

 

 

P√©rim√®tre fonctionnel : Avancement, Limites, Extension. 

Question de comp√©tences : 

Aucune pour l‚Äôinstant 

 

Traitement de cas particuliers :  

 

Avancement de l‚Äôontologie :  

Colonne 

Nombre de ligne avec anomalies ou data √† extraire 

Nombre de lignes trait√©es 

Pourcentage de lignes trait√©es  

üîπ Total lignes automatisables : 368 

Traitement des lignes pour year of experience, automatisable. 

Liste des Analysis_ID des lignes √† atomiser : [175, 176, 177, 178, 329, 330, 515, 516, 517, 518, 519, 520, 521, 522, 523, 527, 528, 607, 608, 609, 610, 611, 612, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 858, 859, 860, 861, 902, 903, 904, 905, 914, 915, 916, 917, 918, 919, 972, 973, 974, 975, 1049, 1050, 1051, 1052, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1232, 1289, 1290, 1291, 1292, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1444, 1445, 1446, 1447, 1448, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1733, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902] 

Id des analyses des lignes pour les pays.  

D√©cal√© de 1 
Analysis_ID concern√©s : [201, 202, 203, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479] 
 
Architecture 

Lien vers la cartographie 

L‚Äôarchitecture de notre ontologie repose sur trois classes primaires : Article, Population et Analysis. Cette structure tripartite permet d‚Äôenglober l‚Äôint√©gralit√© du contenu du fichier source, dans la mesure o√π toutes les donn√©es disponibles peuvent √™tre rattach√©es directement ou indirectement √† l‚Äôune de ces trois entit√©s centrales. 

Cependant, afin de garantir une meilleure granularit√©, modularit√© et scalabilit√©, nous avons choisi d‚Äôenrichir cette structure de base par l‚Äôintroduction de classes compl√©mentaires. Ces classes permettent de mod√©liser plus finement des concepts transversaux ou complexes, tels que le sport pratiqu√©, les mesures statistiques, les m√©diateurs, les mod√©rateurs ou encore les variables psychologiques. Cette approche modulaire facilite la r√©utilisabilit√© des composants de l‚Äôontologie, tout en assurant une meilleure √©volutivit√© du mod√®le dans des contextes d‚Äôanalyse avanc√©e ou de traitement automatique. 

Classe Sport :  

Nous avons introduit la classe Sport parce qu‚Äôelle permet de regrouper de mani√®re coh√©rente et centralis√©e toutes les informations relatives √† une pratique sportive, tout en assurant une meilleure extensibilit√© du mod√®le. En effet, au lieu de rattacher directement les diff√©rentes dimensions du sport (type, niveau, sous-cat√©gorie, etc.) √† la classe Population, l‚Äôutilisation d‚Äôune entit√© Sport permet de construire une structure plus claire, normalis√©e et facilement r√©utilisable. 

Cela pr√©sente plusieurs avantages : 

Modularit√© : la classe Sport agit comme une unit√© autonome pouvant √™tre associ√©e √† diff√©rentes populations. Cela permet de d√©finir un sport une seule fois et de le lier √† plusieurs occurrences, sans dupliquer les informations. 

Granularit√© : les sous-composantes telles que le type de pratique (SportPracticeType), ou la sous-cat√©gorie (SportSubCategory) peuvent √©voluer ind√©pendamment, √™tre enrichies, ou compl√©t√©es par d‚Äôautres m√©tadonn√©es sp√©cifiques. 

Scalabilit√© : le mod√®le devient facilement extensible. De nouvelles propri√©t√©s peuvent √™tre ajout√©es √† la classe Sport sans perturber la structure globale. 

Prenons un exemple concret issu de nos donn√©es pour illustrer la pertinence de la classe Sport. L‚Äôun des articles √©tudi√©s porte sur une population de 200 hommes, √¢g√©s de 18 √† 58 ans, inscrits dans des salles de sport √† Chypre Nord, et pratiquant la musculation en amateur. Ces participants sont d√©crits selon de nombreuses dimensions : sexe, √¢ge, fr√©quence hebdomadaire d‚Äôexercice, ann√©es d‚Äôexp√©rience, type de sport pratiqu√© (individuel, esth√©tique), sous-cat√©gorie (Bodybuilding), ainsi que plusieurs crit√®res li√©s √† leur profil psychologique (motivation, troubles de l'image corporelle, orthorexie, etc.). 

Si l'on avait choisi de rattacher directement toutes ces caract√©ristiques √† la classe Population, le mod√®le aurait rapidement souffert d‚Äôun manque de lisibilit√© et de modularit√©, chaque nouvelle population n√©cessitant une redondance d‚Äôinformations sportives. En introduisant une classe Sport, il devient possible de centraliser l‚Äôensemble des dimensions sp√©cifiques √† la pratique sportive (nom du sport, type de pratique, sous-cat√©gorie, niveau, fr√©quence hebdomadaire, ann√©es d'exp√©rience, etc.) dans une entit√© coh√©rente, qui peut ensuite √™tre li√©e √† une ou plusieurs populations. Ainsi, un sport tel que le Bodybuilding devient une ressource identifiable, r√©utilisable, et extensible, pouvant √™tre enrichie par de nouvelles propri√©t√©s (par exemple : encadrement f√©d√©ral, reconnaissance institutionnelle, risques associ√©s, exigences nutritionnelles, etc.). 

De plus, ce d√©coupage am√©liore la clart√© s√©mantique du graphe RDF. On peut facilement formuler des requ√™tes SPARQL pour interroger toutes les populations li√©es √† un m√™me sport, ind√©pendamment de leurs caract√©ristiques sociod√©mographiques ou psychologiques. Cette structuration favorise √©galement l‚Äôalignement avec d‚Äôautres vocabulaires ou ontologies existants dans le domaine du sport ou de la sant√©. 

D‚Äôautre classe reviennent √† l‚Äô√©vidence comme la classe Genre, ou encore les classes des variables. 

Classe Statistiques : 

Les donn√©es statistiques constituent un √©l√©ment cl√© de notre ontologie. Elles d√©crivent en d√©tail les caract√©ristiques des populations √©tudi√©es dans les articles scientifiques, notamment l‚Äô√¢ge, l‚Äôindice de masse corporelle (IMC), la fr√©quence d‚Äôexercice et les ann√©es d‚Äôexp√©rience. Plut√¥t que d‚Äôint√©grer directement ces valeurs comme de simples propri√©t√©s de la classe Population, nous avons opt√© pour une mod√©lisation plus modulaire, √©volutive et adapt√©e aux traitements futurs, notamment l‚Äôint√©gration de mod√®les de machine learning. 

Pour cela, nous avons introduit des classes sp√©cifiques telles que AgeStatistics, BMIStatistics, ExerciseFrequencyStatistics, ou encore ExperienceStatistics. Chacune de ces classes regroupe des valeurs statistiques comme la moyenne, l‚Äô√©cart-type, les bornes minimale et maximale, ainsi que les unit√©s associ√©es. 

Cette approche repose sur un principe reconnu en mod√©lisation ontologique : le value object pattern, ou r√©ification des mesures. Elle permet de traiter chaque ensemble de donn√©es quantitatives comme un objet structur√©, ce qui favorise : 

une meilleure lisibilit√© du graphe RDF, 

une extensibilit√© facile en cas d‚Äôajout de nouvelles m√©triques, 

une interop√©rabilit√© avec d‚Äôautres vocabulaires, 

et des requ√™tes SPARQL plus riches et cibl√©es. 

Biens√ªr, cette structuration n‚Äôaffaiblit en rien le r√¥le des autres classes du mod√®le, mais s‚Äôinscrit dans une logique de sp√©cialisation et de clart√© conceptuelle 

La qualit√© des donn√©es est un facteur d√©terminant dans tout projet d‚Äôontologie ou de traitement s√©mantique. C‚Äôest pourquoi nous avons mis en place une phase de normalisation et de nettoyage des donn√©es, accompagn√©e d‚Äôune d√©tection des anomalies. Les scripts associ√©s sont disponibles dans le d√©p√¥t Git du projet, sous forme de notebooks reproductibles. Cette √©tape pr√©c√®de toute op√©ration d‚Äôint√©gration dans l‚Äôontologie, afin d‚Äôassurer la coh√©rence, l‚Äôhomog√©n√©it√© et l‚Äôexploitabilit√© des donn√©es RDF. 

 

 

Utilisation de patterns ontologiques 

Certaines entit√©s de notre mod√®le n√©cessitent plus qu‚Äôune simple relation directe pour √™tre repr√©sent√©es de mani√®re pr√©cise. C‚Äôest notamment le cas des mod√©rateurs et m√©diateurs, qui jouent un r√¥le sp√©cifique dans le cadre d‚Äôune analyse statistique donn√©e. Pour refl√©ter correctement cette complexit√©, nous avons eu recours √† des ontology design patterns, et en particulier au pattern dit de "reification of contextual relationships", qui consiste √† introduire une entit√© interm√©diaire entre les ressources. 

Par exemple, lorsqu‚Äôune variable agit comme m√©diateur dans une analyse, mais que la mesure ou le contexte est sp√©cifique √† cette analyse, il est pr√©f√©rable de ne pas cr√©er un lien direct de type hasMediator. √Ä la place, nous mod√©lisons un chemin d‚Äôanalyse (ou path) √† travers une entit√© d√©di√©e, telle que MediationPath. Cette structure permet d‚Äôassocier des informations sp√©cifiques √† une relation donn√©e (comme l‚Äôinstrument de mesure utilis√©), sans alt√©rer la ressource elle-m√™me. 

Cette approche permet de mod√©liser finement les cas o√π une m√™me variable (par exemple, la motivation intrins√®que) est utilis√©e comme m√©diateur dans plusieurs analyses, chacune avec une mesure ou un contexte diff√©rent. Le m√™me principe s‚Äôapplique pour les mod√©rateurs, via une entit√© ModerationPath. 

 

Normalisation de donn√©es  

Canonicalisation des unit√©s. (Exemple avec la fr√©quence d‚Äôexercice) 

Dans un souci d‚Äôhomog√©n√©isation et de clart√© s√©mantique, nous avons adopt√© une convention canonique pour repr√©senter les unit√©s de fr√©quence : le format xxx/yyy, o√π xxx repr√©sente la quantit√© et yyy l‚Äôunit√© de temps. Cette structure, par exemple day/week ou hour/day, permet d‚Äôunifier les multiples formulations linguistiques pr√©sentes dans les donn√©es sources. 

Ainsi, des expressions vari√©es comme : 

"everyday" sont syst√©matiquement converties en 7 days/week, 

"3 hours a day" devient 3 hours/day. 

Ce format permet non seulement de standardiser les valeurs dans les colonnes comme Freq_Unit et Freq_Base, mais √©galement de renforcer la comparabilit√© des donn√©es inter-lignes et leur r√©utilisation dans le cadre d‚Äôanalyses automatiques ou de raisonnements s√©mantiques. 

Par cons√©quent, toutes les nuances de fr√©quence identifi√©es dans les donn√©es brutes doivent √™tre transform√©es manuellement ou par script vers cette repr√©sentation canonique. Cette √©tape est essentielle pour garantir une coh√©rence interne, faciliter les requ√™tes SPARQL, et permettre un enrichissement ult√©rieur de l‚Äôontologie avec des unit√©s quantitatives r√©utilisables. 

Enfin, dans les cas ambigus (par exemple lorsque le texte source indique simplement "everyday" sans pr√©ciser d‚Äôunit√©), la cellule d√©di√©e √† la fr√©quence analys√©e (ExerciseFrequencyForAnalysis) doit √™tre compl√©t√©e par la valeur interpr√©t√©e selon cette convention, accompagn√©e des colonnes associ√©es (Freq_Mean, Freq_Unit, Freq_Base), en veillant √† bien respecter la granularit√© exprim√©e dans les donn√©es originales. 

 

D√©tection des anomalies 

Pour identifier les incoh√©rences, nous avons adopt√© une approche bas√©e sur l‚Äôextraction de motifs r√©currents (pattern matching) √† l‚Äôaide d‚Äôexpressions r√©guli√®res (regex). Cela repose sur le principe de profiling de donn√©es : il s‚Äôagit d‚Äôobserver les formes dominantes dans chaque champ (noms de pays, unit√©s, types d‚Äô√©tudes...) pour en d√©duire un format de r√©f√©rence. Toute d√©viation de ce format est consid√©r√©e comme une anomalie. Contrairement √† des approches plus tol√©rantes, nous avons choisi de ne pas ignorer les erreurs typographiques, car elles nuisent √† la qualit√© s√©mantique et √† l‚Äôinterop√©rabilit√©. Ainsi, m√™me des variations de casse, des accents manquants ou des doublons s√©mantiques sont consid√©r√©s comme des erreurs √† corriger. 

Exemple :  

 

 

Normalisation et nettoyage 

La normalisation repose sur deux pratiques fondamentales : la standardisation lexicale (uniformisation des termes, correction des fautes, harmonisation des formats) et la reconciliation des entit√©s (alignement des variantes vers des formes uniques, √©ventuellement li√©es √† des URI canoniques dans l'ontologie ou dans des r√©f√©rentiels externes). Concr√®tement, cela inclut : 

la suppression d‚Äôespaces ou de ponctuations parasites, 

la mise en minuscule ou majuscule contr√¥l√©e, 

l‚Äôhomog√©n√©isation des dates, unit√©s, noms de sports ou pays, 

et la conversion vers des repr√©sentations s√©mantiques normalis√©es (ex. URI ex:Bodybuilding au lieu de la cha√Æne "Body building"). 

Nous avons privil√©gi√© une approche semi-automatique : les r√®gles sont d√©finies manuellement (bas√©es sur l'analyse des patterns), mais appliqu√©es automatiquement √† l‚Äôensemble des donn√©es √† l‚Äôaide de scripts Python. Ce processus suit les principes du Data Cleaning et de la Data Normalization, tels qu‚Äôils sont d√©crits en ing√©nierie des donn√©es s√©mantiques. Il est indispensable pour garantir la fiabilit√© des inf√©rences, des regroupements par SPARQL, et l‚Äôinterop√©rabilit√© avec d‚Äôautres graphes ou ontologies du domaine. 

Harmonisation des modalit√©s : exemple sur la variable ‚ÄúGender‚Äù 

L‚Äôanalyse des donn√©es brutes a mis en √©vidence une forte h√©t√©rog√©n√©it√© dans les modalit√©s textuelles associ√©es √† certaines variables cat√©gorielles, notamment la variable ‚Äúgender‚Äù. 

 

 Pour garantir l‚Äôinterop√©rabilit√© et la coh√©rence des valeurs dans le graphe RDF, une harmonisation stricte des modalit√©s a √©t√© mise en ≈ìuvre. 

Dans le cas de ‚Äúgender‚Äù, seules trois modalit√©s standardis√©es sont conserv√©es : 

"Male" 

"Female" 

"Other" (pour inclure les identit√©s transgenres, non binaires ou non sp√©cifi√©es) 

Toutes les variations s√©mantiques ou linguistiques d√©tect√©es dans les donn√©es sources, telles que : 

"Man", "Boy", "M", "males" ‚Üí harmonis√©es en "Male" 

"Woman", "Girl", "W", "F", "females" ‚Üí harmonis√©es en "Female" 

ont √©t√© converties √† l‚Äôaide d‚Äôun processus de reconnaissance de motifs textuels via expressions r√©guli√®res (regex) et pattern matching en Python. Ce traitement garantit que chaque valeur de genre est repr√©sent√©e de fa√ßon univoque dans l‚Äôontologie. 

 

 

 

 

Mapping RML/OWL 

‚ÄúBonne pratique‚Äù suivie lors de la r√©daction du fichier de mapping.  

Pour r√©aliser le mapping, nous avons privil√©gi√© l'utilisation d'URI plut√¥t que de valeurs litt√©rales, mais nous avons √©galement utilis√© les valeurs litt√©rales en compl√©ment des URI afin de permettre une interaction homme-machine. 

Nous avons aussi veill√© √† typer les ressources autant que possible, car cela facilite le lien avec les classes dans notre sch√©ma RDF. 

Une collection n'est utilis√©e que lorsqu'elle a une signification propre ; s'il s'agit simplement de repr√©senter une relation avec plusieurs valeurs, alors nous utilisons plusieurs triplets RDF avec la m√™me propri√©t√©. 

Nous avons d√©clar√© des namespaces ; ainsi, au lieu d'inventer de nombreuses URI compl√®tement diff√©rentes, nous utilisons des URI issues d‚Äôun m√™me namespace. 

Dans le cadre de ce projet, nous avons utilis√© RML afin de transformer nos donn√©es, une fois normalis√©es, en ontologies exploitables dans Prot√©g√©. Le fichier de mapping RML est disponible √† l'emplacement suivant : [√† compl√©ter avec le chemin ou le lien]. 

√âtapes du processus de mapping : 

Pr√©paration des donn√©es : 
 Le fichier source doit √™tre au format CSV. 

Conversion du s√©parateur : 
 Un fichier de conversion est n√©cessaire pour garantir que les donn√©es sont bien s√©par√©es par des virgules. Cette √©tape est essentielle, sans quoi le moteur de mapping ne pourra pas interpr√©ter correctement les donn√©es. 

Ex√©cution du script de conversion : 
 Utilisez la commande suivante pour g√©n√©rer un fichier CSV conforme, avec des virgules comme s√©parateurs : 

./script-converter 
 

Configuration du fichier de mapping : 
 Une fois le fichier CSV g√©n√©r√©, indiquez son chemin dans le fichier de mapping RML. 

Ex√©cution du moteur RML : 
 Rendez-vous dans le r√©pertoire contenant le fichier JAR de RMLMapper, puis ex√©cutez la commande suivante : 

java -jar rmlmapper.jar -m chemin/vers/mapping.ttl -o sortie.ttl 
 

Remarque sur l‚Äôatomisation des donn√©es : 

Afin de raffiner davantage la structuration des donn√©es, nous avons proc√©d√© √† une atomisation de certaines colonnes. Par exemple, dans le cas des variables ind√©pendantes, plusieurs valeurs pouvaient √™tre regroup√©es sur une m√™me ligne, ce qui n‚Äôest pas optimal pour un traitement s√©mantique. Pour rem√©dier √† cela, nous avons ajout√© des lignes suppl√©mentaires afin de repr√©senter chaque variable de mani√®re ind√©pendante. Cette approche facilite l‚Äôint√©gration et l‚Äôexploitation des donn√©es dans l'ontologie. 

Int√©gration de l‚Äôontologie √† une interface utilisateur 

Dans le cadre de ce projet, nous avons mis en place une architecture permettant de relier notre ontologie OWL √† une interface utilisateur web. L'objectif est de permettre l‚Äôinterrogation de l‚Äôontologie via des requ√™tes SPARQL et d‚Äôafficher les r√©ponses de mani√®re lisible et interactive, sans manipulation directe des fichiers OWL. 

Cette solution repose sur l'utilisation d‚Äôun triplestore RDF (Apache Jena Fuseki) coupl√© √† une interface web d√©velopp√©e en HTML et JavaScript. 

Pr√©requis 

Pour la mise en ≈ìuvre de cette architecture, les √©l√©ments suivants sont n√©cessaires : 

Java Development Kit (JDK), indispensable pour ex√©cuter le serveur Fuseki. 

Apache Jena Fuseki, utilis√© comme serveur SPARQL pour h√©berger l‚Äôontologie. 

Un fichier OWL repr√©sentant l‚Äôontologie. 

Un √©diteur de texte pour la cr√©ation de l‚Äôinterface (par exemple Visual Studio Code ou Notepad++). 

Un navigateur web pour l‚Äôacc√®s √† l‚Äôinterface Fuseki et √† l‚Äôinterface d√©velopp√©e. 

√âtapes de mise en ≈ìuvre 

Installation de Fuseki et configuration initiale 

T√©l√©charger Apache Jena Fuseki depuis le site officiel : https://jena.apache.org/download/index.cgi. 

Installer Java JDK si ce n‚Äôest pas d√©j√† fait. 

Extraire l‚Äôarchive de Fuseki. 

Lancer le serveur Fuseki en ex√©cutant le fichier fuseki-server (ou fuseki-server.bat sous Windows). 

Acc√©der √† l‚Äôinterface Fuseki via l‚Äôadresse http://localhost:3030. 

Cr√©ation d‚Äôun dataset RDF 

√Ä partir de l‚Äôinterface Web de Fuseki, cr√©er un nouveau dataset. 

Importer le fichier OWL contenant l‚Äôontologie dans ce dataset. 

V√©rifier l‚Äôint√©gration en ex√©cutant des requ√™tes SPARQL simples depuis l‚Äôonglet d√©di√©. 

D√©veloppement de l‚Äôinterface utilisateur 

Cr√©er une page HTML permettant √† l‚Äôutilisateur de d√©clencher des requ√™tes. 

Ajouter un script JavaScript pour envoyer des requ√™tes SPARQL au serveur Fuseki via HTTP. 

Exemple de requ√™te SPARQL ex√©cut√©e √† l‚Äôaide de la m√©thode fetch en JavaScript : 

const query = ` 
  PREFIX ex: <http://example.com/ontology#> 
  SELECT ?variable WHERE { 
    ?s a ex:VariableIndependante ; 
       ex:nom ?variable . 
  } 
`; 
 
fetch("http://localhost:3030/nom_du_dataset/query", { 
  method: "POST", 
  headers: { 
    "Content-Type": "application/sparql-query", 
    "Accept": "application/sparql-results+json" 
  }, 
  body: query 
}) 
.then(response => response.json()) 
.then(data => { 
  console.log(data.results.bindings); 
}); 
 

Les r√©sultats peuvent ensuite √™tre affich√©s dynamiquement dans la page, sous forme de tableau HTML ou de liste. 

 

Utilisation de GraphDB 

 

 

Laisser le tableau tel quel, et faire du process de data lors de la requ√™te, consomme beaucoup (Il faut √™tre sympa pour notre plan√®te) et beaucoup trop long.  

 

J‚Äôai aussi tent√© une s√©paration de sport mais vu que la colonne des sports contient la m√™me info qui se r√©p√®te et bien on peut faire un ‚Äúcontains‚Äù au lieu de tout s√©parer, si la nature de l‚Äôinformation √©tait diff√©rente l√† on aurait privil√©gi√© autre chose.  
 

Pour r√©cup√©rer chaque relation et garder la notion de cette relation appartient √† ce trucs l√†.  
Aussi pour le genre , on peut faire peut √™tre pour la requete un process rapide si on recupere , les femmes bah en r√©alit‚Äù on r√©cup√®re et les femmes et les mixed, on y verra que du feu.  
 
J‚Äôai aussi tent√© l‚Äôintegration d‚ÄôopenAI Api pour int√©rroger l‚Äôontologie , en vrai grave faisable mais autant interroger le tableau excel et le stage tombe √† l‚Äôeau , de plus je ne sais m√™me pas si √ßa va marcher , sachant que le RAG fait partie des derniere nouveaut√© de l‚ÄôIA et que √ßa reste pour l‚Äôinstant une technologie instable.  

 
 

Sachant que je dois avoir le d√©tails du details et sachant qu‚Äôen plus de √ßa je vais devoir obligatoirement atomiser la ligne.  
 

Pourquoi ne pas laisser les donn√©es ¬´ en bloc ¬ª ? 

Traitement de la ligne 395 

Sur la ligne 395, plus de vingt relations sont actuellement regroup√©es dans un m√™me litt√©ral ; or, conserver ces pr√©dicats ¬´ en bloc ¬ª compromet l‚Äôefficacit√© de notre mod√®le RDF : d‚Äôabord, chaque requ√™te SPARQL doit recourir √† un filtrage ou √† une extraction par regex pour isoler la relation recherch√©e, ce qui alourdit la syntaxe et rallonge les temps de r√©ponse ; ensuite, les triplestores n‚Äôindexent que les composantes sujet, pr√©dicat et objet, si bien qu‚Äôune relation enferm√©e dans une cha√Æne n‚Äôest plus indexable et fait perdre tout le b√©n√©fice des index S-P-O optimis√©s ; enfin, en mod√©lisant chaque relation comme un pr√©dicat distinct, on peut la rattacher √† d‚Äôautres vocabulaires normalis√©s, la r√©utiliser dans de futures √©tudes et maintenir ais√©ment le graphe (ajout, correction ou suppression d‚Äôune seule relation sans toucher aux autres), ce qui est impossible lorsque tout est encapsul√© dans un seul litt√©ral. 

 

Le premier bloc principal du mapping, appel√© <#RelationMapping>, cr√©e une ressource RDF de type :Relation pour chaque ligne du fichier CSV. Chaque relation est identifi√©e par un URI unique construit √† partir de l‚Äôidentifiant d‚Äôanalyse, du nom de la variable source et de la variable cible. Cette relation est enrichie avec plusieurs propri√©t√©s : elle a une :source, une :target, un :weight repr√©sentant le coefficient de r√©gression (Œ≤), et une propri√©t√© :partOfAnalysis qui la relie √† une analyse statistique. Les sources et cibles sont elles-m√™mes mod√©lis√©es comme des ressources RDF, permettant une r√©utilisation ou des requ√™tes SPARQL bas√©es sur les variables. 

Le second bloc, <#AnalysisMapping>, cr√©e une ressource RDF de type :Analysis pour chaque identifiant d‚Äôanalyse unique (analysisId) dans le fichier CSV. √Ä chaque analyse sont associ√©es des m√©tadonn√©es descriptives telles que le type d‚Äôanalyse ("single" ou "multiple"), le sport, le genre et l‚Äôann√©e de l‚Äô√©tude. Cela permet de contextualiser chaque relation statistique dans le cadre de l‚Äô√©tude dont elle provient. 

      :source :Drive%20for%20Muscularity ; 

      :target :Bulimic%20Symptomatology ; 

 

 

 

 

 

 

R√©cup√©rer toutes les analyses ; 

PREFIX : <http://example.org/onto#> 

SELECT DISTINCT ?analysis ?analysisId ?type ?sport ?gender ?year 

WHERE { 

  ?analysis a :Analysis ; 

            :analysisId ?analysisId ; 

            :type ?type ; 

            :sport ?sport ; 

            :gender ?gender ; 

            :year ?year . 

} 

ORDER BY ?analysisId 

 

Recuperer toutes les relations  

PREFIX : <http://example.org/onto#> 

  

SELECT DISTINCT 

  ?relation 

  ?source 

  ?target 

  ?weight 

  ?analysisId 

  ?type 

WHERE { 

  ?relation a :Relation ; 

            :source ?source ; 

            :target ?target ; 

            :weight ?weight ; 

            :partOfAnalysis ?analysis . 

  

  ?analysis :analysisId ?analysisId ; 

            :type ?type . 

} 

ORDER BY ?analysisId 

------------------ Recupe relation entre deux variabales--------------------- 

PREFIX : <http://example.org/onto#> 

PREFIX ex: <http://example.org/data#> 

  

SELECT DISTINCT 

  ?relation 

  ?weight 

  ?analysisId 

  ?type 

  ?sport 

  ?gender 

  ?year 

WHERE { 

  ?relation a :Relation ; 

            :source :Drive%20for%20Muscularity ; 

            :target :Bulimic%20Symptomatology ; 

            :weight ?weight ; 

            :partOfAnalysis ?analysis . 

  

  ?analysis :analysisId ?analysisId ; 

            :type ?type ; 

            :sport ?sport ; 

            :gender ?gender ; 

            :year ?year . 

} 

En termes de visuel pour le site graphDB et d‚Äôap√®s leurs documentations, rien ne permet de r√©cup√©rer ce m√™me visuel donc √ßa tombe √† l‚Äôeau, lien vers la documentation API. 

https://graphdb.ontotext.com/documentation/11.0/manage-repos-with-restapi.html 

Du coup je vais tester d‚Äôautre libraire JS pour faire √ßa... mais ce n‚Äôai pas une priorit√© pour l‚Äôinstant √† part si je m‚Äôennui vraiment .... 
Utilisation de Microsoft Visual Basic pour Applicaiton:  

C‚Äôest un outil donn√© par microsoft qui permet de traiter les lignes et colonnes directement dans le tableau tout en utilisant du code, bon le langage est grave chelou mais voil√†, je pense que je pourrais d√©j√† virer pas mal de chose avec √ßa.  
 
Alors la si √ßa passe , c‚Äôest la grosse f√™te , il est ou le bouton executer mdrr ,  
Ah bah trop style √ßa marche wesh. 

Pour taiter la colonne degr√© de relation , voici comment j‚Äôai proc√©d√©... .  
 
Pour les relations simples , si cette derni√®re pr√©sente un infortmations en plus du genre le sexe qui a √©t√© trait√© ce dernier est plac√©e dans sous groupe d‚Äôanalyse et est pris en consid√©ration dans les autres colonnes par exemple dans l‚Äôanalyse d‚Äôid 109 , on retrouve l‚Äôage et pour les aesthetic et les non aesthetic cependant la relation elle n‚Äôest la que pour les aesthetic , su coup l‚Äôage consid√©r√© pour cette analyse et celui des aesthetic ainsi de suite.  
 

Il n‚Äôy a pas eu d‚Äôautre cas pour les relations simple...  

Maintenant passons aux relations multiples, les relations multiples contienne bien plus d‚Äôinformations : 

ATTENTION SI TU SEPRARE UNE CELLULE DE RELATION , SEPARE BIEN LES VI ET VD SOSOSOSOSOSOSOSOS 

La VI si existe  

La VD si existe 

Le moderateur si pr√©sent  

Le mediateur si pr√©sent  

Le R¬≤ si present  

Le rho si pr√©sent  

Le Beta  

Le p  

Un indice de quelque chose  

Des paths si existe  

Des Models si existe  

Des Temps si existe 

Des Step si existe  

Le sexe si  existe 

La population si existe 

Probl√®me C‚Äôest √©cris de mani√®re diff√©rente  

2 √®me probleme mon code ne fait pas la diff entre VI et VD encore moin si moderateur ou mediateur 

Bon bref:  

Analyse 90 => Traitement en fonction de model , et est ce que j‚Äôajoute une colonne pr√©ciser que c‚Äôest une colonne crois√©e.  

 

Et les VI que j‚Äôextrait je vais les mettre dans final VI pour les relations multiples que j‚Äôexplose en relation simple. 

Par quoi on a mod√©lis√©, sur quel crit√®re le model 1 ou 2 a √©t√© fait  

 

Analyse 175 => puis je mettre la VI dans la colonne des VI final 
 

Analyse 204 => Muscle dysmorphie par rapport √† quoi ?  

Analyse 290 , 291 , 293 pr√©sences d‚Äôabr√©viation √ßa ne change rien de toute fa√ßon l‚Äôinfo est dans les autres colonnes.  

Analyse 329 et 330 , pr√©sence d‚Äôabr√©viation  

Analyse 393 pr√©sence de mod√©rateur dans la cellule  

Analyse 395 , pas de vi , mais dans la colonne VI on a toutes les VI , donc premi√®re vi = premier beta etc ... ? 

Analyse 405 => Direct effect , indirect effect ?  

Analyse 413 , mediator , moderateur ou aucun ?  

Du coup pour faire l‚Äôatomisation de la colonne des relations il faut prendre en consid√©ration la colonne des VI, la colonne de m√©diateur et mod√©rateur  

Site Web et comment appr√©hender le code.  

RequirementsAdd commentMore actions 

Node.js is the only requirement. 

Install 

Clone the repository 

In each project inside services/ run npm install to download all the dependencies. 

Note: this command should be run again every time you install/delete a package (which should not happen a lot) 

Run 

In each project, run node index.js so the server will start listening to requests. 
Architecture 

In the services/ folder you will find all the projects that makes your website. Each subfolder is a node.js project that contains: 

A package.json (and potentially some node_modules) 

An index.js file which is an HTTP server able to received requests 

Some logic used for the service to work correctly 

At the start of your project, there are 2 services: 

gateway which is the clients' entry point. It receives all the requests and then redirect them to the correct service. 

files which is used to serve files. It is where your front files will go 

 

To create a new service: 

Create a new folder inside services/ 

Run npm init -y inside the created folder 

Create an index.js containing an HTTP server (and listening to a new port) 

Add any logic you want 

 

To call a service: 

2 external packages are allowed to transfer a request: 

http-proxy allows you to easily transfer a request "as-is" to another HTTP Server (you have an example in the gateway service) 

axios to perform specific REST requests. 

This is the basis of the architecture , so you guess it , whenever I needed a service , I mafe created a specific forlder , with its own port , itw own index.js , the path in the gateway , the dockerfile , and updated the file docker-compose , de plus cet architecture permet de host les site de mani√®re plus facile.  

√âtape 3: Ajouter une config .ttl 

Pour que Fuseki cr√©e automatiquement un dataset (ex: /ds) avec ton ontologie charg√©e d√®s le d√©marrage, tu pourras cr√©er un fichier comme config.ttl ‚Äî je peux te l‚Äô√©crire ensuite si tu veux l'automatisation. 

J‚Äôai trouv√© du code qui peut √™tre utilis√© ici :  

https://jena.apache.org/documentation/fuseki2/fuseki-configuration.html  

du coup je l‚Äôai adapt√© √† notre projet.  

De base j‚Äôai pr√©vu d‚Äôavoir une base de donn√©es pour le projet, la seule utilit√© que je voyais √† cette base de donn√©es, c la mise √† jour et le stockage des versions de l‚Äôontologie, le serveur fuseki fait tr√®s bien l‚Äôaffaire. 

Bien une fois j‚Äôai mes services et que mes services communiquent entre eux, c‚Äôest √† dire que que mon front reussit bien d‚Äôappeler fuseki.. Je peux passer √† la cr√©ation des r√©sultats,  

 

Eh bah on est pas les seuls √† vouloir faire √ßa : https://github.com/dasch-swiss/dsp-das/issues/238 

Bon la y‚Äôa un exemple de ce qu‚Äôon veut faire , comment √ßa des gens ont pens√© au m√™me truc que moi y‚Äôa 8 ans... 
https://github.com/MadsHolten/sparql-visualizer?tab=readme-ov-file 
 

Si jamais je n‚Äôarrive pas √† l‚Äô√©tape du cloud voici les √©tapes pour dockers.  
 
T√©l√©chergez docker desktop depuis ce lien : 

Prendre la version Silicon. Mac 

 https://www.docker.com/products/docker-desktop/ 

Une fois t√©l√©charg√©, lancer docker et acceptez les autorisations.  

Pour ouvrir le terminal depuis un dossier : Mac 

Cliquez sur le dossier dans la barre du chemin d‚Äôacc√®s tout en maintenant la touche Contr√¥le enfonc√©e. 
Sur la petite fen√™tre  

Puis effectuez l‚Äôune des op√©rations suivantes. 

Tout en bas appuyer sur service -> Cliquez Nouveau terminal au dossier.  
 
Le termina s‚Äôouvre :  

V√©rifier la version de docker en √©crivant la commande : docker --version  

Normalement bous avez la version qui s‚Äôafficher.  

Ouvrez un navigateur est saisissez le lien : localhost:8000 
 
Et la-vous ex√©cutez la commande : docker compose up ‚Äìbuild 

Pour ouvrir une nouvelle fen√™tre : Choisissez ¬´ Ouvrir dans Terminal ¬ª. 

Parfait. Maintenant ouvrez un terminal  

Mac : Appuyez sur Command (‚åò) + Espace pour ouvrir Spotlight. 

Tapez terminal. 

Appuyez sur Entr√©e ‚Üí Le Terminal s‚Äôouvre. 

 

Windows :  

Appuyez sur Windows + R ‚Üí tapez cmd ‚Üí appuyez sur Entr√©e. 
 

Maintenant testez que vous avez bien docker avec la commande : docker ‚Äìversion 

Maintenant tester que vous avez bien docker compose : docker compose version 

 

L‚Äôutilisateur vient saisir les informations relatives au sportif, je dois avoir un g√©n√©rateur de requ√™tes SPARQL, un fois la requ√™te cr√©e, elle est transmise au serveur fuseki , qui lui va venir interroger l‚Äôontologie, la r√©ponse attendue est en json, va passer √† travers le g√©n√©rateur de graphe, puis le g√©n√©rateur de tableau, puis le transformateur en csv. L‚Äôutilisateur √† le choix d‚Äôafficher ce qu‚Äôil veut.  

Sachant que le tout doit √™tre orchestr√©e par la Gateway. 

 

Extension future  

https://www.qanswer.ai/fr/tasks/text-to-sparql   

SPARQL Generator Universel 

Un syst√®me qui : 

1. Analyse automatiquement le mapping RML 

javascript 

const mappingAnalyzer = new MappingAnalyzer(rmlFile); 
const ontologyStructure = mappingAnalyzer.extractStructure(); 

2. D√©couvre les entit√©s et relations 

javascript 

// R√©sultat :  
{ 
  entities: ['Article', 'Analysis', 'Population', 'Sport'], 
  properties: { Article: ['title', 'year'], Population: ['gender', 'age'] }, 
  relations: { Article: ['hasAnalysis'], Analysis: ['hasPopulation'] } 
} 

3. G√©n√®re dynamiquement les requ√™tes 

javascript 

const queryBuilder = new DynamicQueryBuilder(ontologyStructure); 
const sparql = queryBuilder.generateQuery(userFilters); 

4. Propose des requ√™tes intelligentes 

D√©tecte automatiquement les filtres possibles 

Sugg√®re des jointures pertinentes 

Adapte les requ√™tes selon le contexte 

 

 

Changer le SPARQL Generator.  
Dans le composant d‚Äôinput , changer le inner html pour le front si besoin.  
Puis changer le code de r√©cup√©ration de donn√©es attention il faut changer le code du search event aussi dans doctor-page.html 
Ensuite il faut aussi changer le code dans doctor.js , dans le playload dans la fonction rechercher toujours dans le component input-interrogation-component 
 

[EF] Quand je clique sur un article, avec un orchestrateur , j‚Äôai une sorte de fiche descriptive de cet article qu‚Äôavec les infos n√©cessaire.  
 
[Task] Fran√ßais ou Anglais?  

 

Il faut un service pour les donn√©es , j‚Äôaime pas qu‚Äôelles soient dans le front 

PREFIX : <http://example.org/onto#> PREFIX ex: <http://example.org/data#> SELECT DISTINCT ?relation ?vi ?vd ?degreR ?degreP ?resultatRelation WHERE { ?relation a :Relations ; :VI ?vi ; :VD ?vd ; :degreR ?degreR . # Relation sp√©cifique entre Well-being et Orthorexia nervosa FILTER(?vi = "Well-being" && ?vd = "Orthorexia nervosa") # Informations suppl√©mentaires optionnelles OPTIONAL { ?relation :degreP ?degreP } OPTIONAL { ?relation :resultatRelation ?resultatRelation } } ORDER BY ?degreR 
 

 

 

 

S√©lecteur principal (7 cat√©gories) :  

Questions g√©n√©rales sur facteurs/ACAD 

Facteurs par type (protecteurs/risque/ambigus) 

Facteurs par cat√©gorie (intra/inter/socio-environnementaux) 

Analyse par caract√©ristiques sportives 

Analyse par profil d'athl√®te 

Outils de mesure 

Questions avanc√©es 

Sous-s√©lecteurs dynamiques selon la cat√©gorie choisie 

Filtres contextuels adapt√©s au choix 

 
 

 

 

  
 

 
 

 

 

 

  

 

 
 
 
 

 
Bon c mignon , t‚Äôa une interface √† moiti√© fonctionelle et un ontologie quasi finie.  
 
Maintenant , il faut ajouter le double click pour pouvoir recuperer les informations concerant les variables.  
Mais je ne sais honnetement pas si je dois faire la recuperation depuis le csv ou depuis fuseki , en soit , je fais la requete sparql ,  
J'ai d√©velopp√© un script Python qui automatise la cr√©ation de ma taxonomie RDF √† partir du fichier CSV de hi√©rarchie. Ce script constitue la premi√®re brique de mon syst√®me IA-DAS pour l'analyse des donn√©es sur le sport et la psychologie. 

Le processus de transformation que j'ai mis en place lit le fichier Class_hierarchy_V1Hierarchy.csv qui contient 672 lignes de concepts organis√©s hi√©rarchiquement. Mon script parcourt chaque ligne et extrait les relations entre les classes sur 5 niveaux de profondeur maximum. Les noms des concepts, souvent compos√©s de plusieurs mots avec des espaces et caract√®res sp√©ciaux, sont convertis en identifiants URI conformes aux standards du web s√©mantique. 

La g√©n√©ration du fichier Turtle suit une structure logique claire. D'abord, j'√©tablis les pr√©fixes n√©cessaires pour mon namespace ia-das.org ainsi que les vocabulaires standards comme RDFS et OWL. Ensuite, j'identifie les classes racines qui n'ont pas de parent dans la hi√©rarchie, notamment les cinq grandes cat√©gories : DEAB, Intrapersonal factor related to DEAB, Other behaviors, Interpersonal factor related to DEAB, et Sociocultural factor related to DEAB. Pour chaque classe, je maintiens √† la fois l'identifiant URI technique et le label humainement lisible. 

L'organisation des relations de subsomption utilise la propri√©t√© rdfs:subClassOf pour cr√©er l'arbre taxonomique. Chaque concept enfant est li√© √† son parent direct, permettant ainsi √† Fuseki de faire des inf√©rences sur l'appartenance des instances aux super-classes. Par exemple, quand j'indique qu'une variable est de type "Supplementation", Fuseki comprendra automatiquement qu'elle appartient aussi √† "MuscleDysmorphia", "DEAB" et toutes les classes parentes. 

Le fichier de sortie ia-das-taxonomy.ttl g√©n√©r√© contient environ 600 classes et autant de relations hi√©rarchiques. Cette taxonomie servira de colonne vert√©brale s√©mantique pour toutes mes analyses ult√©rieures et permettra des requ√™tes SPARQL sophistiqu√©es exploitant la hi√©rarchie des concepts. 

Les 3 fichiers et leur r√¥le 

1. ia-das-taxonomy.ttl (√âtape 1) C'est mon dictionnaire de concepts. Il d√©finit que "Binge eating" est un sous-type de "Eating disorders" qui est lui-m√™me un sous-type de "DEAB". Comme un arbre g√©n√©alogique des concepts psychologiques. 

2. ia-das-ontology.ttl (√âtape 2) C'est le sch√©ma de ma base de donn√©es. Il dit : "Un Article a un titre, des auteurs, peut avoir plusieurs Analyses, chaque Analyse a une Population, des Variables, etc." C'est la structure de mes donn√©es. 

3. ia-das-mapping.ttl (√âtape 3 - ce qu'on vient de faire) C'est la recette de transformation. Il dit : "La colonne 'DOI' de mon Excel devient la propri√©t√© bibo:doi, la colonne 'Analysis_ID' cr√©e une nouvelle entit√© Analysis, etc." C'est le pont entre mon Excel et le graphe de connaissances. 

 
Rapport d'analyse du mapping RML - Syst√®me IA-DAS 

Introduction 

Dans le cadre du d√©veloppement du syst√®me de knowledge graph IA-DAS pour l'analyse des donn√©es de recherche en psychologie du sport, j'ai proc√©d√© √† l'analyse de notre fichier de mapping RML. Ce document pr√©sente la logique globale de transformation de nos donn√©es CSV vers le format RDF. 

Architecture de transformation 

Notre mapping RML agit comme un traducteur intelligent qui convertit nos donn√©es tabulaires de recherche en un graphe de connaissances RDF structur√© et interconnect√©. 

Approche adopt√©e 

Articles scientifiques comme fondation J'ai choisi de structurer le mapping autour des articles acad√©miques, utilisant la norme BIBO (Bibliographic Ontology) comme standard reconnu. Chaque ligne de notre dataset CSV g√©n√®re un article acad√©mique qui sert de n≈ìud central pour toutes les informations associ√©es √† l'√©tude. 

Analyses comme unit√©s de mesure Chaque Analysis_ID de notre dataset devient une instance d'analyse autonome qui encapsule : 

Les relations statistiques entre variables psychologiques 

Les conclusions et interpr√©tations des auteurs 

Les limites m√©thodologiques identifi√©es 

Les perspectives de recherche sugg√©r√©es 

Les variables mod√©ratrices et m√©diatrices 

R√©f√©rencement taxonomique intelligent Notre mapping adopte une approche optimis√©e : plut√¥t que de recr√©er la hi√©rarchie taxonomique de 672 concepts, il r√©f√©rence directement les concepts existants dans notre taxonomie. Les variables ind√©pendantes (VI) et d√©pendantes (VD) pointent vers ces concepts pr√©-d√©finis, √©vitant ainsi la duplication et garantissant la coh√©rence. 

Structure d'interconnexion 

La logique de notre mapping suit un mod√®le hi√©rarchique clair : 

Entit√©s principales cr√©√©es 

Population et contexte d√©mographique Chaque analyse est associ√©e √† une population d'√©tude avec ses caract√©ristiques d√©mographiques d√©taill√©es, permettant des analyses par segments de population. 

Contexte sportif sp√©cialis√© Le mapping structure le contexte sportif (discipline, niveau de pratique, type d'entra√Ænement) comme entit√© autonome, facilitant les analyses comparatives entre sports. 

Relations statistiques formalis√©es Les r√©sultats statistiques (coefficients de corr√©lation r, niveaux de significativit√© p, coefficients de r√©gression beta) sont transform√©s en entit√©s "Relations" structur√©es qui connectent formellement les variables entre elles. 

Objectifs atteints 

Cette architecture de mapping me permet de : 

Pr√©server l'int√©grit√© des donn√©es : Toutes les informations du dataset original sont conserv√©es et structur√©es 

Faciliter les requ√™tes complexes : La structure RDF permet des questions sophistiqu√©es via SPARQL 

Maintenir la coh√©rence taxonomique : Le r√©f√©rencement vers notre taxonomie existante √©vite les incoh√©rences 

Optimiser les performances : L'approche de r√©f√©rencement plut√¥t que de duplication am√©liore l'efficacit√© 

Cas d'usage enabled 

Gr√¢ce √† ce mapping, notre syst√®me permettra des requ√™tes analytiques avanc√©es telles que : 

"Identifier tous les facteurs psychologiques corr√©l√©s aux troubles alimentaires chez les athl√®tes f√©minines de haut niveau" 

"Analyser les relations entre estime de soi et performance dans les sports individuels versus collectifs" 

"Cartographier les mod√©rateurs statistiques des relations motivation-performance par cat√©gorie d'√¢ge" 

Conclusion 

Ce mapping RML constitue le c≈ìur de la transformation de nos donn√©es de recherche disparates en un r√©seau de connaissances coh√©rent et interrogeable. Il respecte les standards du web s√©mantique tout en pr√©servant la richesse sp√©cifique de notre domaine de recherche en psychologie du sport. 


La dans ma requ√™te, j‚Äôai 90 r√©sultats, parfois les r√©sultats se superpose, je dois √™tre capable de voir chaque relation √† elle seule avec les couleurs rouges si √† risque , vert si protecteur et gris si rien.  
 

Probleme de timeout  

Rapport d‚ÄôOptimisation du G√©n√©rateur de Requ√™tes SPARQL 

Contexte 

Le g√©n√©rateur actuel construit des requ√™tes SPARQL dynamiques avec des filtres appliqu√©s en dehors des blocs OPTIONAL et utilise des filtres co√ªteux (ex. LCASE) ainsi qu‚Äôune gestion non optimis√©e des limites et de la pagination. Ces √©l√©ments ralentissent l‚Äôex√©cution et alourdissent les requ√™tes. 

 

Objectifs 

R√©duire la charge des filtres co√ªteux. 

Optimiser l‚Äô√©valuation des filtres en limitant leur port√©e. 

Impl√©menter une pagination efficace pour √©viter de charger trop de donn√©es en une seule requ√™te. 

 

Solutions Propos√©es 

1. Remplacer les filtres FILTER(LCASE(...)) par des triples directs 

Probl√®me : Utilisation de FILTER(LCASE(...)) pour comparer des valeurs textuelles, ce qui est co√ªteux. 

Solution : 
 Au lieu de filtrer sur une variable avec FILTER(LCASE(str(?gender)) = LCASE("female")), il faut directement matcher le triple : 

sparql 

CopierModifier 

?population iadas:gender "female" . 
 

Avantages : 

√âvaluation plus rapide par Fuseki. 

Moins de traitement sur les variables. 

 

2. Int√©grer les filtres dans les blocs OPTIONAL correspondants 

Probl√®me : Les filtres sont appliqu√©s globalement, souvent √† l‚Äôext√©rieur des blocs OPTIONAL, ce qui g√©n√®re des erreurs ou des scans inutiles. 

Solution : 
 Inclure les filtres √† l‚Äôint√©rieur des blocs OPTIONAL o√π les variables sont garanties d‚Äôexister : 

sparql 

CopierModifier 

OPTIONAL { 
  ?population iadas:ageStats ?ageStats . 
  ?ageStats iadas:minAge ?minAge ; 
            iadas:maxAge ?maxAge ; 
            iadas:meanAge ?meanAge . 
  FILTER((?minAge <= 25 && ?maxAge >= 18) || (!BOUND(?minAge) && !BOUND(?maxAge))) 
} 
 

Avantages : 

Les filtres s‚Äôappliquent uniquement quand les donn√©es sont pr√©sentes. 

R√©duction du co√ªt des filtres sur des variables non d√©finies. 


3. Ajouter une gestion de la pagination via LIMIT et OFFSET 

Probl√®me : Extraction massive de donn√©es (ex. LIMIT 10000), qui surcharge la m√©moire et ralentit les r√©ponses. 

Solution : 

Fixer une taille de page (ex. 200 r√©sultats). 

Utiliser LIMIT et OFFSET dans les requ√™tes successives. 

Faire des appels it√©ratifs c√¥t√© client/serveur pour r√©cup√©rer toutes les donn√©es progressivement. 

sparql 

CopierModifier 

ORDER BY ?analysis ?relation 
LIMIT 200 OFFSET 0 
 

Puis augmenter OFFSET de 200 √† chaque nouvel appel. 

Avantages : 

R√©duction de la charge m√©moire sur le serveur. 

Am√©lioration de la r√©activit√© c√¥t√© client. 

 

R√©sum√© du d√©veloppement de la page Questions de Comp√©tences 

üéØ Objectif du projet 

Cr√©er une nouvelle page pour interroger l'ontologie avec des questions pr√©d√©finies sur les comp√©tences ACAD (Activit√©s et Conduites Addictives), en utilisant un syst√®me d'accord√©on au lieu de filtres complexes. 

3. Conception de l'interface accord√©on 

Question 1 : "Pour une ACAD sp√©cifique..." ‚Üí S√©lection directe (pas de sous-options) 

Question 2 : "Pour un facteur sp√©cifique..." ‚Üí 3 sous-options (Protecteur/Risque/Ambigu) 

Question 3 : "Pour une cat√©gorie de facteurs..." ‚Üí 4 sous-options (Intrapersonnels/Interpersonnels/Socio-environnementaux/Autres) 

4. D√©veloppement du composant Web Component 

Pourquoi Web Component : R√©utilisabilit√© et encapsulation comme les autres composants du projet 

Logique : Gestion des clics, ouverture/fermeture d'accord√©ons, s√©lection des questions 

√âv√©nements : √âmission d'√©v√©nements personnalis√©s vers la page parent 

5. Optimisation de l'interface 

Texte en blanc : Coh√©rence avec le design existant 

Tailles r√©duites : Police plus petite pour garder la compacit√© de l'autre page 

Scroll au lieu de grow : Hauteurs fixes (120px accord√©on, 80px contenu) pour √©viter l'agrandissement 

Boutons optimis√©s : Plac√©s √† droite de la question sur la m√™me ligne pour √©conomiser l'espace vertical 

6. Structure des donn√©es √©chang√©es 

Quand une question est s√©lectionn√©e, le composant √©met un √©v√©nement avec : 

javascript 

{ 
    queryType: 'predefined_competence', 
    questionId: 'q1' | 'q2-protecteur' | 'q3-intrapersonnels' | etc., 
    questionText: "Texte complet de la question", 
    description: "Description d√©taill√©e" 
} 

 

üîß Prochaines √©tapes : Int√©gration c√¥t√© serveur 

1. Adaptation du serveur SPARQL (port 8003) 

Nouvel endpoint : Cr√©er /api/competence pour traiter les questions pr√©d√©finies 

Diff√©rence avec /api/query : Au lieu de recevoir des filtres complexes, re√ßoit juste un questionId 

Logique : Mapper chaque questionId vers une requ√™te SPARQL pr√©d√©finie 

2. Mapping des questions vers requ√™tes SPARQL 

Cr√©er une correspondance comme : 

javascript 

const COMPETENCE_QUERIES = { 
    'q1': 'SELECT ?facteur ?categorie WHERE { ... }', 
    'q2-protecteur': 'SELECT ?acad WHERE { ?facteur rdf:type :Protecteur ... }', 
    'q3-intrapersonnels': 'SELECT ?acad WHERE { ?facteur rdf:type :Intrapersonnel ... }', 
    // etc. 
} 

3. Format de r√©ponse attendu 

Le serveur doit retourner la m√™me structure que l'API existante : 

javascript 

{ 
    head: { vars: [...] }, 
    results: { bindings: [...] }, 
    sparqlQuery: "...", // Requ√™te g√©n√©r√©e pour affichage 
    performance: { ... } // Optionnel 
} 

4. Tests et validation 

Frontend : La page affiche d√©j√† les erreurs de connexion 

Backend : Tester chaque questionId et s'assurer que les requ√™tes SPARQL retournent des r√©sultats coh√©rents 

Int√©gration : V√©rifier que les graphiques et tableaux s'affichent correctement 

5. Points d'attention 

Gestion d'erreurs : Le frontend affiche d√©j√† des messages d'erreur clairs si le serveur ne r√©pond pas 

Performance : Les requ√™tes pr√©d√©finies devraient √™tre plus rapides que les filtres dynamiques 

√âvolutivit√© : Facile d'ajouter de nouvelles questions en ajoutant des entr√©es dans le mapping 

 

 

 

 

 

   PREFIX xsd: <http://www.w3.org/2001/XMLSchema#> 

sparql-generator  |    

sparql-generator  | SELECT DISTINCT ?analysis ?relation ?vi ?vd ?resultatRelation ?moderator ?mediator ?categoryVI ?categoryVD ?sportName ?gender ?minAge ?maxAge ?meanAge ?minFreq ?maxFreq ?meanFreq ?minExp ?maxExp ?meanExp WHERE { 

sparql-generator  |   ?analysis a iadas:Analysis . 

sparql-generator  |   ?analysis iadas:hasRelation ?relation . 

sparql-generator  |   ?relation iadas:hasDependentVariable ?variableVD . 

sparql-generator  |   ?variableVD iadas:VD ?vd . 

sparql-generator  |   ?variableVD iadas:hasCategory ?categoryVD .                                                 

sparql-generator  |   FILTER(LCASE(str(?categoryVD)) = LCASE("DEAB")) 

sparql-generator  |    

sparql-generator  |   OPTIONAL {  

sparql-generator  |     ?relation iadas:hasIndependentVariable ?variableVI . 

sparql-generator  |     ?variableVI iadas:VI ?vi . 

sparql-generator  |     ?variableVI iadas:hasCategory ?categoryVI . 

sparql-generator  |   } 

sparql-generator  |   OPTIONAL {  

sparql-generator  |     ?analysis iadas:hasSport ?sport . 

sparql-generator  |     ?sport iadas:sportName ?sportName . 

sparql-generator  |   } 

sparql-generator  |   OPTIONAL {  

sparql-generator  |     ?analysis iadas:hasPopulation ?population . 

sparql-generator  |     ?population iadas:gender ?gender . 

sparql-generator  |   } 

sparql-generator  |   OPTIONAL { ?relation iadas:resultatRelation ?resultatRelation } 

sparql-generator  |   OPTIONAL { ?analysis iadas:hasModerator ?moderator } 

sparql-generator  |   OPTIONAL { ?analysis iadas:hasMediator ?mediator } 

sparql-generator  |   FILTER((!BOUND(?resultatRelation) && !BOUND(?sportName) && !BOUND(?gender)) || (LCASE(str(?sportName)) = LCASE("Gymnastics")) && (!BOUND(?resultatRelation) && !BOUND(?sportName) && !BOUND(?gender)) || (LCASE(str(?gender)) = LCASE("female"))) 

sparql-generator  | } 
 

 

---------------------------------------------------------- 

rator  | Filters received: { queryType: 'generated', sportType: 'Gymnastics', gender: 'female' }       

sparql-generator  | üéØ G√©n√©ration des filtres d√©mographiques: { queryType: 'generated', sportType: 'Gymnastics', gender: 'female' }                                                                                                

sparql-generator  | üéØ OPTIMISATION: Requ√™te centr√©e sur sport 

sparql-generator  | REQU√äTE OPTIMIS√âE SPORT COMPL√àTE: 

sparql-generator  |                                                                                               

sparql-generator  |     PREFIX iadas: <http://ia-das.org/onto#> 

sparql-generator  |     PREFIX iadas-data: <http://ia-das.org/data#>                                              

sparql-generator  |     PREFIX xsd: <http://www.w3.org/2001/XMLSchema#> 

sparql-generator  |                                                                                               

sparql-generator  | SELECT DISTINCT ?analysis ?relation ?vi ?vd ?resultatRelation ?moderator ?mediator ?categoryVI ?categoryVD ?sportName ?gender ?minAge ?maxAge ?meanAge ?minFreq ?maxFreq ?meanFreq ?minExp ?maxExp ?meanExp WHERE { 

sparql-generator  |   ?analysis a iadas:Analysis . 

sparql-generator  |   ?analysis iadas:hasSport ?sport .                                                           

sparql-generator  |   ?sport iadas:sportName ?sportName .                                                         

sparql-generator  |   FILTER(LCASE(str(?sportName)) = LCASE("Gymnastics"))                                        

sparql-generator  |    

sparql-generator  |   OPTIONAL {                                                                                  

sparql-generator  |     ?analysis iadas:hasRelation ?relation .                                                   

sparql-generator  |     ?relation iadas:hasIndependentVariable ?variableVI . 

sparql-generator  |     ?variableVI iadas:VI ?vi .                                                                

sparql-generator  |     ?variableVI iadas:hasCategory ?categoryVI . 

sparql-generator  |   }                                                                                           

sparql-generator  |   OPTIONAL {                                                                                  

sparql-generator  |     ?relation iadas:hasDependentVariable ?variableVD . 

sparql-generator  |     ?variableVD iadas:VD ?vd .                                                                

sparql-generator  |     ?variableVD iadas:hasCategory ?categoryVD . 

sparql-generator  |   }                                                                                           

sparql-generator  |   OPTIONAL {                                                                                  

sparql-generator  |     ?analysis iadas:hasPopulation ?population . 

sparql-generator  |     ?population iadas:gender ?gender .                                                        

sparql-generator  |   }                                                                                           

sparql-generator  |   OPTIONAL { ?relation iadas:resultatRelation ?resultatRelation } 

sparql-generator  |   OPTIONAL { ?analysis iadas:hasModerator ?moderator }                                        

sparql-generator  |   OPTIONAL { ?analysis iadas:hasMediator ?mediator } 

sparql-generator  |   FILTER(!BOUND(?gender) || LCASE(str(?gender)) = LCASE("female"))                            

sparql-generator  | }                                                                                             

sparql-generator  | ORDER BY ?analysis ?relation                                                                  

sparql-generator  | LIMIT 10000 

sparql-generator  | üöÄ Sending complete query to Fuseki...     ,  

 

Un autre truc qui fait perdre les donn√©es est les valeurs NA dans le tableau. En soit dans le tableau √ßa a du sens mais pour l‚Äôontologie non. 

 

G√©n√©ral 

Changer les noms pour refl√©ter le projet dans les pr√©fixes. 

Ajouter la d√©finition de l‚Äôontologie principale. 

Chaque classe et propri√©t√© doit avoir un label et un commentaire. 

Pour chaque propri√©t√©, indiquer le domaine et le range. 

Articles / Bibliographie 

Utiliser la classe Article comme sous-classe de bibo:Article. 

Pas besoin de red√©finir toutes les propri√©t√©s ; utiliser directement celles de Bibo. 

Ajouter un label et un commentaire pour chaque propri√©t√© pertinente. 

G√©rer les DOI et autres identifiants uniques via des URI sp√©cifiques. 

Populations / Analyses 

Le nombre de population doit √™tre d√©fini dans la classe Population. 

Les d√©tails sp√©cifiques seront dans les sous-classes ou entit√©s li√©es, par exemple Sous-Analyse. 

IHM / Interfaces 

Pour les sports ou autres cat√©gories, int√©grer le nom directement dans l‚ÄôURI pour plus de clart√©. 

Hi√©rarchies et r√®gles de mapping 

Cr√©er les hi√©rarchies une seule fois, en dehors de la construction des donn√©es, pour √©viter de les recr√©er √† chaque fois. 

Les r√®gles de mapping doivent √™tre s√©par√©es de la structure principale. 

Restructuration de l'ontologie IA-DAS : Analyse des am√©liorations apport√©es 

Fondements m√©thodologiques et d√©finition formelle de l'ontologie 

La premi√®re √©tape de restructuration a consist√© √† √©tablir une d√©finition formelle de l'ontologie IA-DAS selon les standards du Web s√©mantique. Cette d√©marche fondamentale, qui √©tait absente dans la version initiale, comprend d√©sormais la d√©claration explicite de l'ontologie comme ressource OWL avec ses m√©tadonn√©es essentielles : titre bilingue (fran√ßais/anglais), description d√©taill√©e du domaine d'application, identification des cr√©ateurs, et versioning. Cette formalisation constitue le socle indispensable pour assurer la d√©couvrabilit√©, la r√©utilisabilit√© et l'interop√©rabilit√© de l'ontologie dans l'√©cosyst√®me du Web s√©mantique, tout en respectant les bonnes pratiques √©tablies par le W3C. 

Documentation exhaustive et multilinguisme 

L'am√©lioration la plus significative r√©side dans l'ajout syst√©matique de documentation pour chaque √©l√©ment ontologique. Chaque classe et propri√©t√© dispose d√©sormais d'un label (rdfs:label) en fran√ßais et en anglais, accompagn√© d'un commentaire explicatif (rdfs:comment) d√©taillant sa fonction et son utilisation. Cette documentation bilingue r√©pond √† plusieurs objectifs strat√©giques : faciliter la compr√©hension par des utilisateurs internationaux, assurer la maintenance √† long terme de l'ontologie, et permettre une r√©utilisation efficace par d'autres √©quipes de recherche. Cette approche documentaire transforme l'ontologie d'un simple sch√©ma technique en une ressource scientifique accessible et compr√©hensible. 

Sp√©cification rigoureuse des domaines et port√©es 

La d√©finition pr√©cise des domaines (rdfs:domain) et des port√©es (rdfs:range) pour chaque propri√©t√© constitue une avanc√©e majeure dans la rigueur conceptuelle de l'ontologie. Cette sp√©cification formelle √©limine les ambigu√Øt√©s d'interpr√©tation en √©tablissant clairement quelles classes peuvent utiliser chaque propri√©t√© et quels types de valeurs sont accept√©s. L'utilisation appropri√©e des types de donn√©es XSD (xsd:integer pour les effectifs, xsd:decimal pour les statistiques, xsd:string pour les textes) renforce la coh√©rence s√©mantique et facilite la validation des donn√©es. Cette formalisation permet √©galement aux outils de raisonnement automatique de d√©tecter les incoh√©rences et d'inf√©rer de nouvelles connaissances. 

Int√©gration optimis√©e avec l'ontologie BIBO 

La restructuration de la gestion bibliographique repr√©sente une am√©lioration conceptuelle majeure. Plut√¥t que de red√©finir des propri√©t√©s d√©j√† existantes, l'approche adopt√©e cr√©e une classe iadas:SportPsychologyArticle comme sous-classe de bibo:AcademicArticle, h√©ritant ainsi de toutes les propri√©t√©s bibliographiques standard. Cette strat√©gie d'extension plut√¥t que de red√©finition respecte le principe de r√©utilisation des ontologies existantes, √©vite la duplication conceptuelle, et assure l'interop√©rabilit√© avec d'autres syst√®mes bibliographiques. Les propri√©t√©s sp√©cifiques au domaine de la psychologie du sport (pays d'√©tude, type d'√©tude) sont ajout√©es uniquement lorsqu'elles n'existent pas dans BIBO, cr√©ant une sp√©cialisation coh√©rente et non redondante. 

Clarification de la structure populationnelle 

La mod√©lisation des populations d'√©tude a √©t√© repens√©e pour s√©parer clairement les informations g√©n√©rales des statistiques d√©taill√©es. La classe iadas:Population contient d√©sormais directement les informations fondamentales comme la taille d'√©chantillon (sampleSize) et la distribution par genre, conform√©ment aux recommandations initiales. Les statistiques descriptives sp√©cialis√©es (√¢ge, IMC, fr√©quence d'exercice, ann√©es d'exp√©rience) sont mod√©lis√©es comme des classes s√©par√©es li√©es √† la population principale. Cette architecture modulaire facilite les requ√™tes, am√©liore la lisibilit√© conceptuelle, et permet une extension future de nouveaux types de statistiques sans modification de la structure de base. 

Optimisation des identifiants URI 

La simplification des sch√©mas d'URI constitue une am√©lioration significative de la lisibilit√© et de la maintenabilit√©. Les URI complexes incluant des noms de sports ont √©t√© remplac√©es par des identifiants simples bas√©s sur l'ID d'analyse, les noms √©tant stock√©s comme propri√©t√©s (sportName). Cette approche pr√©sente plusieurs avantages : URI plus courtes et lisibles, √©limination des probl√®mes d'encodage li√©s aux caract√®res sp√©ciaux dans les noms, facilitation de la gestion des donn√©es, et coh√©rence avec les meilleures pratiques du Web s√©mantique. Cette modification n'affecte pas la fonctionnalit√© mais am√©liore consid√©rablement l'ergonomie du syst√®me. 

Architecture modulaire et extensibilit√© 

La nouvelle structure ontologique adopte une approche modulaire qui facilite les extensions futures. Les classes de statistiques sp√©cialis√©es (AgeStatistics, BMIStatistics, ExerciseFrequencyStatistics, YearsOfExperienceStatistics) illustrent cette modularit√©, permettant l'ajout de nouveaux types de mesures sans impacter la structure existante. Cette architecture respecte les principes d'√©volutivit√© et de maintenabilit√© essentiels pour un projet de recherche √† long terme, tout en pr√©servant la compatibilit√© avec les requ√™tes SPARQL existantes. 

Conservation de la compatibilit√© et migration 

Malgr√© ces am√©liorations substantielles, une attention particuli√®re a √©t√© port√©e √† la pr√©servation de la compatibilit√© avec l'infrastructure existante. Tous les noms de propri√©t√©s et les r√©f√©rences aux colonnes du CSV sont conserv√©s, garantissant que les requ√™tes SPARQL d√©j√† d√©velopp√©es continuent de fonctionner sans modification. Cette approche de migration progressive permet de b√©n√©ficier imm√©diatement des am√©liorations ontologiques tout en pr√©servant l'investissement technique r√©alis√©, illustrant une d√©marche pragmatique de d√©veloppement incr√©mental conforme aux contraintes d'un projet de recherche op√©rationnel. 

Rapport de Troubleshooting - D√©ploiement API SPARQL sur AWS 

Contexte du Probl√®me 

Projet : IA-DAS (Intelligence Artificielle - Data Analysis System) Date : 16 ao√ªt 2025 Environnement : AWS EC2 avec Docker Compose Erreur initiale : Failed to fetch lors des appels API depuis le frontend 

Sympt√¥mes Observ√©s 

Erreur Frontend : TypeError: Failed to fetch at rechercherCompetence (competence-page.js:105:32) 

Comportement : 

L'application fonctionne parfaitement en local (localhost) 

√âchec des requ√™tes API sur l'environnement AWS (51.44.188.162) 

Timeout sur les appels vers http://51.44.188.162:8003 

Investigation Technique 

Analyse du Code Probl√®me identifi√© : Incoh√©rence dans la d√©tection d'environnement 

Code probl√©matique : // URL d√©finie mais non utilis√©e const apiUrl = window.location.hostname === 'localhost' ? 'http://localhost:8003' : http://${window.location.hostname}:8003; 

// Hardcoding dans le fetch const response = await fetch('http://51.44.188.162:8003', { method: 'POST', // ... }); 

V√©rification de l'Infrastructure 

Test de connectivit√© locale : curl http://localhost:8003/ R√©sultat : "M√©thode non autoris√©e" (serveur r√©pond) 

V√©rification des conteneurs Docker : docker ps Confirmation : sparql-generator actif sur port 8003 

Analyse des logs Docker : sparql-generator | Succ√®s: 5/6 requ√™tes sparql-generator | FUSEKI EST MAINTENANT CHAUD ! 

Diagnostic R√©seau 

Test externe : curl http://51.44.188.162:8003/ R√©sultat : Connection timeout 

V√©rification des ports expos√©s : 

docker-compose.yml 

sparql-generator: ports: - "8003:8003" # Port correctement expos√© 

Cause Racine 

AWS Security Groups bloquant le port 8003 

Le serveur SPARQL fonctionnait correctement en interne, mais le pare-feu AWS emp√™chait l'acc√®s externe au port 8003. 

Solution Appliqu√©e 

Correction du Code JavaScript function getApiUrl() { const hostname = window.location.hostname; if (hostname === 'localhost' || hostname === '127.0.0.1') { return 'http://localhost:8003'; } else { return http://${hostname}:8003; } } 

// Utilisation coh√©rente const response = await fetch(getApiUrl(), { method: 'POST', headers: { 'Content-Type': 'application/json' }, body: JSON.stringify(payload) }); 

Configuration AWS Security Groups Action : Ajout d'une r√®gle d'entr√©e 

Type : Custom TCP 

Port : 8003 

Source : 0.0.0.0/0 

Description : SPARQL API access 

Impl√©mentation de Debug Avanc√© Ajout de logs d√©taill√©s pour faciliter le diagnostic futur : console.log("URL API utilis√©e:", apiUrl); console.log("Payload envoy√©:", JSON.stringify(payload)); console.log("Status HTTP:", response.status); 

Validation de la Solution 

Tests de Validation : 

Test local : curl http://localhost:8003/ ‚Üí "M√©thode non autoris√©e" (OK) 

Test externe : http://51.44.188.162:8003 ‚Üí "M√©thode non autoris√©e" (OK) 

Test fonctionnel : Requ√™tes POST depuis le frontend ‚Üí Succ√®s 

R√©sultats : 

R√©solution compl√®te du probl√®me "Failed to fetch" 

Application fonctionnelle sur les deux environnements 

D√©tection automatique localhost/AWS op√©rationnelle 

Recommandations 

Pr√©vention : 

Checklist de d√©ploiement : V√©rifier syst√©matiquement les Security Groups AWS 

Tests d'int√©gration : Inclure des tests de connectivit√© externe 

Documentation : Maintenir une liste des ports n√©cessaires par service 

Am√©liorations : 

Configuration centralis√©e : Utiliser des variables d'environnement pour les URLs d'API 

Monitoring : Impl√©menter des health checks pour les services 

Gestion d'erreurs : Am√©liorer les messages d'erreur utilisateur 

Temps de R√©solution 

Temps total : ~2 heures 

Identification de la cause : 1h30 

Impl√©mentation de la solution : 30 minutes 

Conclusion 

Le probl√®me √©tait d√ª √† une configuration r√©seau AWS incompl√®te plut√¥t qu'√† un d√©faut applicatif. La mise en place d'outils de diagnostic d√©taill√©s a permis d'identifier rapidement la cause racine et d'appliquer la solution appropri√©e. 

Cette exp√©rience souligne l'importance de v√©rifier l'ensemble de la cha√Æne de d√©ploiement (code, infrastructure, r√©seau) lors du diagnostic de probl√®mes de connectivit√©. 

 

Rapport d'analyse : Choix de conception pour l'interface d'interrogation IA-DAS 

Introduction 

Ce rapport pr√©sente les justifications scientifiques et m√©thodologiques qui sous-tendent les choix de conception de l'interface d'interrogation pour le syst√®me IA-DAS (Intelligence Artificielle - D√©sordres Alimentaires Sportifs). Cette interface vise √† faciliter l'exploration des donn√©es de recherche en psychologie du sport, particuli√®rement dans le domaine des troubles du comportement alimentaire chez les athl√®tes. 

1. Cat√©gorisation des sports par risque de troubles alimentaires 

1.1 Classification scientifique des sports 

La litt√©rature scientifique a √©tabli une classification des sports bas√©e sur le risque de d√©veloppement de troubles du comportement alimentaire (TCA). Cette classification distingue principalement les sports "lean" (√† risque) des sports "non-lean" (√† moindre risque), avec des sous-cat√©gories sp√©cifiques : 

Sports √† haut risque ("lean sports") : 

Sports esth√©tiques : gymnastique, patinage artistique, danse, natation synchronis√©e 

Sports d'endurance : course de fond, cyclisme, natation longue distance 

Sports √† cat√©gories de poids : lutte, boxe, judo, aviron 

Sports anti-gravitationnels : saut en hauteur, saut √† la perche, escalade 

Sports √† risque mod√©r√© : 

Sports techniques : golf, tennis, tir √† l'arc 

Sports de ballon : football, basketball, volleyball 

Sports de puissance : sprint, lancer 

1.2 Justification √©pid√©miologique 

Les √©tudes √©pid√©miologiques d√©montrent que la pr√©valence des TCA chez les athl√®tes √©lites (13,5%) est significativement sup√©rieure √† celle de la population g√©n√©rale (4,6%). Dans les sports esth√©tiques, cette pr√©valence peut atteindre 42% chez les femmes, contre 16% dans les sports de ballon. Cette diff√©rence substantielle justifie l'inclusion de filtres sp√©cifiques par type de sport dans l'interface. 

2. Cat√©gorisation des variables d√©pendantes (ACAD) 

2.1 Classification des Attitudes et Comportements Alimentaires Dysfonctionnels 

La cat√©gorisation des variables d√©pendantes suit la taxonomie √©tablie par la recherche en psychologie du sport pour les troubles alimentaires : 

Cat√©gorie DEAB (Disordered Eating and Athletic Behaviors) : 

Restriction alimentaire 

Comportements compensatoires 

Pr√©occupations excessives li√©es au poids 

Exercice compulsif 

Cette cat√©gorie constitue le c≈ìur de l'ontologie IA-DAS car elle repr√©sente directement les manifestations comportementales des troubles alimentaires dans le contexte sportif. 

2.2 Autres manifestations psychologiques 

L'interface inclut √©galement d'autres cat√©gories de variables d√©pendantes pour permettre une exploration exhaustive : 

Image corporelle et satisfaction corporelle 

Anxi√©t√© de performance 

Perfectionnisme adaptatif/maladaptatif 

Estime de soi 

3. Cat√©gorisation des variables ind√©pendantes (facteurs) 

3.1 Facteurs intrapersonnels 

Les caract√©ristiques psychologiques individuelles incluent l'estime de soi, l'image corporelle et l'anxi√©t√© physique sociale, qui constituent des facteurs de risque √©tablis. Cette cat√©gorie englobe : 

Traits de personnalit√© (perfectionnisme, traits obsessionnels-compulsifs) 

Facteurs cognitifs (distorsions cognitives, croyances dysfonctionnelles) 

R√©gulation √©motionnelle 

Historique de troubles mentaux 

3.2 Facteurs interpersonnels 

Les variables li√©es √† l'environnement social sportif, notamment la pression sociale pour la minceur de la part de l'environnement sportif, constituent des pr√©dicteurs significatifs des comportements alimentaires dysfonctionnels. Cette cat√©gorie comprend : 

Relations avec les entra√Æneurs 

Dynamiques d'√©quipe 

Support social familial 

Relations avec les pairs athl√®tes 

3.3 Facteurs socio-environnementaux 

L'environnement sportif peut inclure des commentaires de co√©quipiers, d'entra√Æneurs, de parents ou de juges, ainsi que du public. Ces facteurs englobent : 

Culture organisationnelle du club/f√©d√©ration 

Pression m√©diatique et sociale 

Normes culturelles li√©es au corps 

Facteurs socio-√©conomiques 

3.4 Autres comportements 

Cette cat√©gorie capture les comportements qui peuvent √™tre associ√©s aux troubles alimentaires sans en √™tre des manifestations directes : 

Comportements de sant√© g√©n√©raux 

Habitudes de vie (sommeil, hydratation) 

Consommation de substances 

Comportements compensatoires non alimentaires 

4. Cat√©gorisation des caract√©ristiques d√©mographiques 

4.1 Genre et vuln√©rabilit√© diff√©rentielle 

Les √©tudes montrent que les femmes athl√®tes, particuli√®rement les jeunes femmes, et les athl√®tes impliqu√©s dans des sports associ√©s √† la minceur sont plus enclins aux troubles alimentaires et √† l'insatisfaction corporelle. L'interface permet donc de filtrer par : 

Genre (Homme/Femme/Mixte) 

Interaction genre √ó type de sport 

4.2 √Çge et p√©riodes critiques 

Les sports esth√©tiques sont consid√©r√©s comme des sports √† entr√©e pr√©coce o√π les jeunes filles se sp√©cialisent souvent d√®s l'√¢ge de huit ans, ce qui signifie que dans de nombreux sports, les filles grandissent devant la communaut√© sportive. L'interface propose des cat√©gories d'√¢ge bas√©es sur les p√©riodes d√©veloppementales critiques : 

Adolescents (10-17 ans) : p√©riode de d√©veloppement identitaire 

Jeunes adultes (18-25 ans) : transition vers l'√¢ge adulte 

Adultes (26-40 ans) : stabilisation identitaire 

Adultes matures (41-65 ans) : maintien et d√©clin 

4.3 Exp√©rience sportive et sp√©cialisation 

Les liens entre les indicateurs psychologiques n√©gatifs tels que les troubles alimentaires et la sp√©cialisation sportive pr√©coce sont document√©s. Les cat√©gories d'exp√©rience permettent d'explorer ces relations : 

D√©butant (< 2 ans) : phase d'apprentissage 

Interm√©diaire (2-7 ans) : d√©veloppement des comp√©tences 

Exp√©riment√© (7-15 ans) : expertise et sp√©cialisation 

Expert (15+ ans) : ma√Ætrise et potentiel d√©clin 

4.4 Fr√©quence d'entra√Ænement et intensit√© 

Les applications de fitness et de r√©gime peuvent avoir des cons√©quences n√©gatives non intentionnelles, notamment chez les √©tudiants universitaires √† risque de troubles alimentaires. Les cat√©gories de fr√©quence d'exercice permettent d'identifier les seuils critiques : 

Faible (< 5h/semaine) : participation r√©cr√©ative 

Mod√©r√©e (5-10h/semaine) : engagement r√©gulier 

√âlev√©e (10-15h/semaine) : entra√Ænement intensif 

Intensive (15+ h/semaine) : risque de surentra√Ænement 

5. Types de relations statistiques 

5.1 Relations protectrices (-) 

Ces relations indiquent des facteurs qui r√©duisent le risque de d√©velopper des troubles alimentaires. Les recherches montrent que certains facteurs peuvent servir de protection contre le d√©veloppement de comportements alimentaires dysfonctionnels. 

5.2 Relations de risque (+) 

Les facteurs de risque pour les comportements alimentaires dysfonctionnels chez les athl√®tes n√©cessitent une √©tude dans des designs √† mesures r√©p√©t√©es pour pouvoir commencer √† pr√©dire le d√©veloppement des sympt√¥mes. 

5.3 Relations ambigu√´s (NS - Non Significatives) 

Ces relations capturent la complexit√© des interactions entre variables, o√π les effets peuvent varier selon les contextes ou les sous-populations. 

6. Justifications technologiques et d'usage 

6.1 Interface adaptative 

L'interface propose des champs personnalisables ("custom") pour permettre aux chercheurs de d√©finir des crit√®res sp√©cifiques non couverts par les cat√©gories pr√©d√©finies. Cette flexibilit√© est essentielle car en raison des ressources limit√©es dans ce type de recherche, il y a un manque d'inclusivit√© compl√®te √† travers les disciplines sportives, les genres, les races et les niveaux de comp√©tence sportive. 

6.2 Autocompl√©tition et facilit√© d'usage 

Le syst√®me d'autocompl√©tion bas√© sur les donn√©es r√©elles de l'ontologie garantit la coh√©rence des requ√™tes et r√©duit les erreurs de saisie. Cette approche est particuli√®rement importante dans un domaine o√π il existe un besoin de programmes √©ducatifs am√©lior√©s pour donner √† ceux qui ne sont pas inform√©s une meilleure compr√©hension de la fa√ßon dont les facteurs psychologiques, sociaux et relationnels influencent ceux qui souffrent de troubles alimentaires. 

7. Limitations et perspectives 

7.1 Limitations actuelles 

En raison de l'h√©t√©rog√©n√©it√© significative dans les conceptions de recherche et les mesures des √©tudes incluses, une m√©ta-analyse des r√©sultats cl√©s n'a pas √©t√© r√©alis√©e. Cette limitation se refl√®te dans la n√©cessit√© de maintenir une interface flexible capable de g√©rer des taxonomies √©volutives. 

7.2 √âvolutions futures 

L'interface devra √©voluer pour int√©grer de nouvelles cat√©gorisations √©mergentes de la recherche, notamment les facteurs li√©s aux r√©seaux sociaux et aux technologies num√©riques, qui montrent une influence croissante sur les troubles alimentaires chez les athl√®tes. 

Conclusion 

Les choix de conception de l'interface d'interrogation IA-DAS s'appuient sur une base scientifique solide √©tablie par plus de deux d√©cennies de recherche en psychologie du sport. La cat√©gorisation propos√©e refl√®te les consensus actuels de la communaut√© scientifique tout en maintenant la flexibilit√© n√©cessaire pour accommoder l'√©volution des connaissances dans ce domaine en constante expansion. Cette approche garantit que l'interface serve efficacement les besoins des chercheurs, cliniciens et professionnels du sport dans leur mission de compr√©hension et de pr√©vention des troubles alimentaires chez les athl√®tes. 

 

 

‚óè Logique de filtrage d√©mographique pour les cat√©gories pr√©d√©finies 

  

  Le syst√®me de recherche IA-DAS impl√©mente une logique de filtrage d√©mographique sophistiqu√©e pour maximiser la pertinence des 

  r√©sultats lors de l'utilisation des cat√©gories pr√©d√©finies. Cette approche reconna√Æt que les √©tudes scientifiques rapportent 

  les caract√©ristiques des populations de mani√®re h√©t√©rog√®ne : certaines fournissent des plages d'√¢ge (ex: 18-25 ans), d'autres 

  des moyennes (ex: √¢ge moyen = 22 ans), et parfois les deux. 

  

  Principe de chevauchement inclusif : Lorsqu'un utilisateur s√©lectionne une cat√©gorie pr√©d√©finie comme "Jeune adulte (18-25 

  ans)", le syst√®me g√©n√®re une requ√™te SPARQL avec une clause UNION qui capture deux types d'analyses. Premi√®rement, il r√©cup√®re 

   toutes les √©tudes dont les plages d√©mographiques chevauchent avec la cat√©gorie recherch√©e, utilisant la logique suivante : 

  une plage chevauche si l'√¢ge maximum de la population √©tudi√©e est sup√©rieur ou √©gal au minimum recherch√© ET si l'√¢ge minimum 

  de la population est inf√©rieur ou √©gal au maximum recherch√©. Deuxi√®mement, il inclut toutes les √©tudes dont les valeurs 

  moyennes tombent dans la plage de la cat√©gorie s√©lectionn√©e. 

  

  Application multi-crit√®res : Cette logique s'applique uniform√©ment aux trois crit√®res d√©mographiques disponibles : l'√¢ge (avec 

   les cat√©gories adolescent, jeune adulte, adulte, adulte mature, senior), la fr√©quence d'exercice (faible < 5h/semaine, 

  mod√©r√©e 5-10h, √©lev√©e 10-15h, intensive 15+h), et les ann√©es d'exp√©rience (d√©butant < 2 ans, interm√©diaire 2-7 ans, 

  exp√©riment√© 7-15 ans, expert 15+ ans). Cette approche garantit une couverture maximale des donn√©es disponibles tout en 

  maintenant la coh√©rence scientifique des r√©sultats. 

  

  Diff√©renciation avec les filtres personnalis√©s : Le syst√®me distingue automatiquement entre les cat√©gories pr√©d√©finies et les 

  filtres personnalis√©s. Lorsqu'un utilisateur saisit des valeurs pr√©cises dans les champs personnalis√©s, la recherche utilise 

  une logique plus restrictive qui ne consid√®re que les correspondances exactes ou les moyennes dans une plage de ¬±1 unit√©, 

  offrant ainsi une pr√©cision maximale pour les recherches sp√©cialis√©es. 

R√©sum√© de l'impl√©mentation s√©curis√©e 

J'ai impl√©ment√© un syst√®me d'authentification s√©curis√© pour prot√©ger l'acc√®s √† la mise √† jour de l'ontologie. Voici ce qui a √©t√© r√©alis√© : 

C√¥t√© serveur (Gateway) : J'ai ajout√© un syst√®me d'authentification JWT avec hashage SHA-256 du mot de passe. Le serveur expose une nouvelle route /api/auth qui v√©rifie le mot de passe hash√© et g√©n√®re un token JWT valide 30 minutes. L'acc√®s √† update-page.html est d√©sormais prot√©g√© par v√©rification du token. 

C√¥t√© client : J'ai remplac√© le bouton direct par une modal d'authentification √©l√©gante qui s'affiche avant l'acc√®s. Le syst√®me utilise sessionStorage pour stocker temporairement le token (plus s√©curis√© que localStorage). Une fois authentifi√©, l'utilisateur acc√®de √† la page de mise √† jour, et le token est automatiquement supprim√© √† la fermeture du navigateur. 

S√©curit√© impl√©ment√©e : Mot de passe hash√© c√¥t√© serveur, tokens JWT avec expiration, protection CORS, validation c√¥t√© client et serveur, et nettoyage automatique des sessions. Le mot de passe par d√©faut est "password" (hash: 5e884898da28047151d0e56f8dc6292773603d0d6aabbdd62a11ef721d1542d8) mais peut √™tre configur√© via les variables d'environnement ADMIN_PASSWORD_HASH et JWT_SECRET. 


------------------------------------------------------------------------------------- 

  Le syst√®me d'authentification impl√©ment√© pr√©sente une s√©curit√© de niveau interm√©diaire adapt√©e √† un environnement 

   de recherche contr√¥l√©. La protection repose sur un m√©canisme JWT c√¥t√© serveur avec hashage SHA-256 du mot de 

  passe et une v√©rification c√¥t√© client via sessionStorage. Cependant, plusieurs vuln√©rabilit√©s ont √©t√© identifi√©es 

   : l'acc√®s direct √† update-page.html n'est prot√©g√© que c√¥t√© client (contournable en d√©sactivant JavaScript), le 

  token peut √™tre manipul√© directement dans le sessionStorage du navigateur, et aucune limitation de tentatives de 

  connexion n'est en place contre les attaques par force brute. De plus, l'utilisation d'un mot de passe par d√©faut 

   faible ("password") et l'absence de salage dans le hashage constituent des risques suppl√©mentaires. Pour un 

  environnement de production, il serait recommand√© d'impl√©menter une protection c√¥t√© serveur avec sessions 

  authentifi√©es, d'ajouter une limitation des tentatives de connexion, et d'utiliser bcrypt avec salt pour le 

  hashage des mots de passe. N√©anmoins, pour le contexte actuel d'un projet de recherche avec acc√®s restreint, 

  cette solution offre un niveau de s√©curit√© acceptable tout en maintenant une simplicit√© d'utilisation appropri√©e. 

 

 